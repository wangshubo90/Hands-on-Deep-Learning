{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import SimpleITK as sitk # to read nii files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv3D = partial(keras.layers.Conv3D, kernel_size=3, strides=(1,)*3,\n",
    "        padding=\"SAME\", use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(keras.layers.Layer):\n",
    "    # separate construction and execution\n",
    "    # be aware of the strides' shape\n",
    "    def __init__(self, filters, strides=(1,)*3, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.ResSubUnit = 2\n",
    "        \n",
    "        # a list a layers that can be iterated\n",
    "        self.main_layers = [\n",
    "                keras.layers.BatchNormalization(),\n",
    "                self.activation,\n",
    "                DefaultConv3D(filters, strides=strides, kernel_initializer=\"he_normal\"),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                self.activation,\n",
    "                DefaultConv3D(filters, strides=(1,)*3, kernel_initializer=\"he_normal\"),\n",
    "                ]\n",
    "        self.skip_layers = []\n",
    "        if np.prod(strides) > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv3D(filters, kernel_size=1, strides=strides, kernel_initializer=\"he_normal\")\n",
    "                ]\n",
    "            \n",
    "            \n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        orig_x = inputs\n",
    "        \n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        for layer in self.skip_layers:\n",
    "            orig_x = layer(orig_x)\n",
    "        \n",
    "        return x + orig_x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResidualUnit, self).get_config()\n",
    "        config.update({'ResSubUnit': self.ResSubUnit})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = (16, 32, 64, 128)\n",
    "strides = (1, 2, 2, 2)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(DefaultConv3D(filters[0], kernel_size=3, strides=(1,)*3,\n",
    "        input_shape=[48, 108, 108, 1], kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.MaxPool3D(pool_size=(2,)*3, padding=\"SAME\"))\n",
    "\n",
    "for filter, stride in zip(filters[1:], strides[1:]):\n",
    "    model.add(ResidualUnit(filter, strides=(stride,)*3))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.GlobalAvgPool3D())\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"sgd\",\n",
    "        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320,)\n"
     ]
    }
   ],
   "source": [
    "# read the file references\n",
    "f = 'File_references.csv'\n",
    "\n",
    "file_df = pd.read_csv(f)\n",
    "file_df.head()\n",
    "\n",
    "subject_ID = file_df['Sample'].to_numpy()\n",
    "file_name = file_df['File name'].to_numpy()\n",
    "label = file_df['Perforation'].to_numpy()\n",
    "datapath = r\"/home/spl/Machine Learning/Data\"\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 48, 108, 108, 1)\n"
     ]
    }
   ],
   "source": [
    "imgs = np.zeros(shape = (len(file_name),48,108,108))\n",
    "for i, f in enumerate(file_name):\n",
    "    img = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(datapath,f)))\n",
    "    img = img / 255.0\n",
    "    imgs[i] = img\n",
    "    \n",
    "imgs = np.expand_dims(imgs,axis = -1)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 48, 108, 108, 1)\n",
      "(48, 48, 108, 108, 1)\n",
      "(48, 48, 108, 108, 1)\n",
      "41\n",
      "12\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training set and test set by a ratio of 7:3\n",
    "train_img, test_img, train_l, test_l = train_test_split(\n",
    "        imgs, label, test_size=0.3, random_state=41)\n",
    "\n",
    "# split test data into validation and evaluation evenly\n",
    "val_img, evl_img, val_l, evl_l = train_test_split(\n",
    "        test_img, test_l, test_size = 0.5, random_state=41)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(val_img.shape)\n",
    "print(evl_img.shape)\n",
    "\n",
    "print(train_l.sum())\n",
    "print(val_l.sum())\n",
    "print(evl_l.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 48, 108, 108, 1]\n",
      "[12]\n"
     ]
    }
   ],
   "source": [
    "def read_dataset(imgs, labels, batch_size=8,shuffle_size=224):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "    dataset = dataset.shuffle(shuffle_size).batch(batch_size)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "train_set = read_dataset(train_img, train_l, batch_size, shuffle_size = 224)\n",
    "val_set = read_dataset(val_img, val_l, batch_size, shuffle_size = 48)\n",
    "evl_set = read_dataset(evl_img, evl_l, batch_size, shuffle_size = 48)\n",
    "\n",
    "for line in train_set.take(1):\n",
    "    print(line[0].shape.as_list())\n",
    "    print(line[1].shape.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"4th_batch_only.h5\",\n",
    "        save_best_only=True)\n",
    "\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f} \\n\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"4th batch only\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 18 steps, validate for 4 steps\n",
      "Epoch 1/40\n",
      " 1/18 [>.............................] - ETA: 1:01 - loss: 0.8543 - accuracy: 0.4167WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.114086). Check your callbacks.\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.6251 - accuracy: 0.6520\n",
      "val/train: 0.78 \n",
      "\n",
      "18/18 [==============================] - 6s 331ms/step - loss: 0.6239 - accuracy: 0.6528 - val_loss: 0.4886 - val_accuracy: 0.7500\n",
      "Epoch 2/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4238 - accuracy: 0.8200\n",
      "val/train: 1.14 \n",
      "\n",
      "18/18 [==============================] - 3s 157ms/step - loss: 0.4167 - accuracy: 0.8255 - val_loss: 0.4736 - val_accuracy: 0.7500\n",
      "Epoch 3/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4088 - accuracy: 0.8100\n",
      "val/train: 1.14 \n",
      "\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 0.4042 - accuracy: 0.8160 - val_loss: 0.4565 - val_accuracy: 0.7708\n",
      "Epoch 4/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3385 - accuracy: 0.8600\n",
      "val/train: 1.32 \n",
      "\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 0.3357 - accuracy: 0.8632 - val_loss: 0.4445 - val_accuracy: 0.7917\n",
      "Epoch 5/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3777 - accuracy: 0.8250\n",
      "val/train: 1.16 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.3779 - accuracy: 0.8255 - val_loss: 0.4399 - val_accuracy: 0.8333\n",
      "Epoch 6/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3193 - accuracy: 0.8600\n",
      "val/train: 1.40 \n",
      "\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 0.3123 - accuracy: 0.8679 - val_loss: 0.4391 - val_accuracy: 0.8750\n",
      "Epoch 7/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2823 - accuracy: 0.9000\n",
      "val/train: 1.54 \n",
      "\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.2948 - accuracy: 0.8915 - val_loss: 0.4550 - val_accuracy: 0.8958\n",
      "Epoch 8/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2671 - accuracy: 0.9000\n",
      "val/train: 1.80 \n",
      "\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.2609 - accuracy: 0.9057 - val_loss: 0.4671 - val_accuracy: 0.9167\n",
      "Epoch 9/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.3014 - accuracy: 0.8900\n",
      "val/train: 1.69 \n",
      "\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.2985 - accuracy: 0.8868 - val_loss: 0.5043 - val_accuracy: 0.9375\n",
      "Epoch 10/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2662 - accuracy: 0.8900\n",
      "val/train: 1.97 \n",
      "\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.2717 - accuracy: 0.8868 - val_loss: 0.5356 - val_accuracy: 0.8542\n",
      "Epoch 11/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2645 - accuracy: 0.9050\n",
      "val/train: 2.20 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.2571 - accuracy: 0.9104 - val_loss: 0.5692 - val_accuracy: 0.6458\n",
      "Epoch 12/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2488 - accuracy: 0.9150\n",
      "val/train: 2.36 \n",
      "\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.2514 - accuracy: 0.9104 - val_loss: 0.5886 - val_accuracy: 0.6042\n",
      "Epoch 13/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2117 - accuracy: 0.9250\n",
      "val/train: 2.76 \n",
      "\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.2131 - accuracy: 0.9245 - val_loss: 0.5925 - val_accuracy: 0.5833\n",
      "Epoch 14/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2309 - accuracy: 0.9050\n",
      "val/train: 2.54 \n",
      "\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.2356 - accuracy: 0.9009 - val_loss: 0.6014 - val_accuracy: 0.5833\n",
      "Epoch 15/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2562 - accuracy: 0.8950\n",
      "val/train: 2.50 \n",
      "\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.2555 - accuracy: 0.8962 - val_loss: 0.6221 - val_accuracy: 0.5417\n",
      "Epoch 16/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1934 - accuracy: 0.9250\n",
      "val/train: 2.88 \n",
      "\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.1964 - accuracy: 0.9245 - val_loss: 0.5687 - val_accuracy: 0.6042\n",
      "Epoch 17/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2579 - accuracy: 0.9000\n",
      "val/train: 2.41 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.2675 - accuracy: 0.8962 - val_loss: 0.6411 - val_accuracy: 0.5208\n",
      "Epoch 18/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1972 - accuracy: 0.9500\n",
      "val/train: 3.09 \n",
      "\n",
      "18/18 [==============================] - 2s 131ms/step - loss: 0.1906 - accuracy: 0.9528 - val_loss: 0.5734 - val_accuracy: 0.6042\n",
      "Epoch 19/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2052 - accuracy: 0.9167\n",
      "val/train: 2.56 \n",
      "\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 0.2053 - accuracy: 0.9198 - val_loss: 0.5248 - val_accuracy: 0.7083\n",
      "Epoch 20/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1909 - accuracy: 0.9314\n",
      "val/train: 2.51 \n",
      "\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.1912 - accuracy: 0.9306 - val_loss: 0.4807 - val_accuracy: 0.7708\n",
      "Epoch 21/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1850 - accuracy: 0.9500\n",
      "val/train: 2.30 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1852 - accuracy: 0.9481 - val_loss: 0.4304 - val_accuracy: 0.8333\n",
      "Epoch 22/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1675 - accuracy: 0.9500\n",
      "val/train: 2.36 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1690 - accuracy: 0.9481 - val_loss: 0.4003 - val_accuracy: 0.8542\n",
      "Epoch 23/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1858 - accuracy: 0.9350\n",
      "val/train: 1.95 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1805 - accuracy: 0.9387 - val_loss: 0.3547 - val_accuracy: 0.8750\n",
      "Epoch 24/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1572 - accuracy: 0.9450\n",
      "val/train: 1.85 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1595 - accuracy: 0.9434 - val_loss: 0.2966 - val_accuracy: 0.8958\n",
      "Epoch 25/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2009 - accuracy: 0.9300\n",
      "val/train: 1.60 \n",
      "\n",
      "18/18 [==============================] - 2s 122ms/step - loss: 0.1964 - accuracy: 0.9340 - val_loss: 0.3131 - val_accuracy: 0.9167\n",
      "Epoch 26/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1686 - accuracy: 0.9550\n",
      "val/train: 1.54 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1683 - accuracy: 0.9575 - val_loss: 0.2608 - val_accuracy: 0.9167\n",
      "Epoch 27/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1491 - accuracy: 0.9650\n",
      "val/train: 1.55 \n",
      "\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.1703 - accuracy: 0.9528 - val_loss: 0.2677 - val_accuracy: 0.8958\n",
      "Epoch 28/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1601 - accuracy: 0.9500\n",
      "val/train: 1.57 \n",
      "\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 0.1575 - accuracy: 0.9528 - val_loss: 0.2393 - val_accuracy: 0.9375\n",
      "Epoch 29/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1605 - accuracy: 0.9350\n",
      "val/train: 1.50 \n",
      "\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.1582 - accuracy: 0.9387 - val_loss: 0.2318 - val_accuracy: 0.9375\n",
      "Epoch 30/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1615 - accuracy: 0.9400\n",
      "val/train: 1.34 \n",
      "\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 0.1689 - accuracy: 0.9340 - val_loss: 0.2275 - val_accuracy: 0.9375\n",
      "Epoch 31/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1669 - accuracy: 0.9450\n",
      "val/train: 1.32 \n",
      "\n",
      "18/18 [==============================] - 2s 134ms/step - loss: 0.1657 - accuracy: 0.9481 - val_loss: 0.2184 - val_accuracy: 0.9167\n",
      "Epoch 32/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1289 - accuracy: 0.9650\n",
      "val/train: 1.58 \n",
      "\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.1285 - accuracy: 0.9623 - val_loss: 0.2051 - val_accuracy: 0.9375\n",
      "Epoch 33/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1443 - accuracy: 0.9500\n",
      "val/train: 1.35 \n",
      "\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 0.1392 - accuracy: 0.9528 - val_loss: 0.1899 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.9500\n",
      "val/train: 1.39 \n",
      "\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.1414 - accuracy: 0.9434 - val_loss: 0.1977 - val_accuracy: 0.9167\n",
      "Epoch 35/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1221 - accuracy: 0.9700\n",
      "val/train: 1.59 \n",
      "\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.1186 - accuracy: 0.9717 - val_loss: 0.1897 - val_accuracy: 0.9375\n",
      "Epoch 36/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9550\n",
      "val/train: 1.37 \n",
      "\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.1374 - accuracy: 0.9528 - val_loss: 0.1863 - val_accuracy: 0.9375\n",
      "Epoch 37/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1408 - accuracy: 0.9500\n",
      "val/train: 1.35 \n",
      "\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 0.1378 - accuracy: 0.9528 - val_loss: 0.1888 - val_accuracy: 0.9167\n",
      "Epoch 38/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.9657\n",
      "val/train: 1.54 \n",
      "\n",
      "18/18 [==============================] - 2s 122ms/step - loss: 0.1304 - accuracy: 0.9670 - val_loss: 0.2040 - val_accuracy: 0.9375\n",
      "Epoch 39/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1136 - accuracy: 0.9706\n",
      "val/train: 1.74 \n",
      "\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 0.1103 - accuracy: 0.9722 - val_loss: 0.1915 - val_accuracy: 0.9167\n",
      "Epoch 40/40\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1239 - accuracy: 0.9550\n",
      "val/train: 1.58 \n",
      "\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 0.1190 - accuracy: 0.9575 - val_loss: 0.1883 - val_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8c34b9cd50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, steps_per_epoch=224 // batch_size, epochs=40,\n",
    "          validation_data=val_set,\n",
    "          validation_steps=48 // batch_size,\n",
    "          callbacks=[checkpoint_cb,  \n",
    "                     PrintValTrainRatioCallback(), tensorboard_cb]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(evl_set, steps= 48 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is an inplementation of dataset pipline using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_reader(file_name, subject_ID, label):\n",
    "    def generator():\n",
    "        for f, ID, y in zip(file_name,subject_ID, label):\n",
    "            img = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(datapath,f)))\n",
    "            img = img / 255.0\n",
    "            img = np.expand_dims(img, axis = -1).astype(np.float32)\n",
    "\n",
    "            y = np.expand_dims(y, axis = -1).astype(np.float32)\n",
    "\n",
    "            yield img,y\n",
    "    return generator # return a generator function \n",
    "\n",
    "# an example of construct tf.data.Dataset using from_generator()\n",
    "\n",
    "'''file_generator = simple_reader(file_name, subject_ID, label)\n",
    "filepath_dataset = tf.data.Dataset.from_generator(file_generator, \n",
    "        (tf.float32,tf.float32))\n",
    "\n",
    "filepath_dataset = filepath_dataset.repeat(3).shuffle(160).batch(batch_size)\n",
    "\n",
    "\n",
    "for line in filepath_dataset.take(1):\n",
    "    print(line[0].shape.as_list())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(file_name, subject_ID, label, shuffle_size, batch_size):\n",
    "    file_generator = simple_reader(file_name, subject_ID, label)\n",
    "    dataset = tf.data.Dataset.from_generator(file_generator, \n",
    "            (tf.float32,tf.float32))\n",
    "    dataset = dataset.repeat(3)\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_f, test_f, train_ID, test_ID, train_l, test_l = train_test_split(\n",
    "        file_name, subject_ID, label, test_size=0.3, random_state=42)\n",
    "\n",
    "val_f, evl_f, val_ID, evl_ID, val_l, evl_l = train_test_split(\n",
    "        test_f, test_ID, test_l, test_size = 0.5, random_state=42)\n",
    "\n",
    "print(train_f.shape)\n",
    "print(val_f.shape)\n",
    "print(evl_f.shape)\n",
    "    \n",
    "train_set = reader(train_f, train_ID, train_l, 224, 8)\n",
    "val_set = reader(val_f, val_ID, val_l, 48, 8)\n",
    "evl_set = reader(evl_f, evl_ID, evl_l, 48, 8)\n",
    "\n",
    "for line in train_set.take(2):\n",
    "    print(line[0].shape.as_list())\n",
    "    print(line[1].shape.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
